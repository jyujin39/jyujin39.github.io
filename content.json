{"meta":{"title":"Data Science YJ","subtitle":"my daily study blog for Data Science","description":null,"author":"Yujin Jeon","url":"https://jyujin39.github.io"},"pages":[],"posts":[{"title":"classification models","slug":"classification-models","date":"2018-11-28T12:17:36.000Z","updated":"2018-11-28T12:41:58.579Z","comments":true,"path":"2018/11/28/classification-models/","link":"","permalink":"https://jyujin39.github.io/2018/11/28/classification-models/","excerpt":"","text":"분류모형분류(classification)는 독립 변수 값이 주어졌을 때 그 독립 변수 값과 가장 연관성이 큰 종속변수 카테고리(클래스)를 계산하는 문제이다. 분류 모형의 종류 판별함수(discriminant function) 모형 : 주어진 데이터를 서로 다른 영역으로 나누는 경계면을 찾는다. 확률적 모형 확률적 판별(discriminative) 모형 : 주어진 데이터가 특정 카테고리일 조건부확률을 직접 계산한다. 확률적 생성(generative) 모형 : 주어진 데이터가 특정 카테고리일 조건부확률을 베이즈정리를 통해 계산한다. 모형 방법론 Linear/Quadratic Discriminant Analysis 확률적 생성 모형 나이브 베이지안 (Naive Bayes) 확률적 생성 모형 로지스틱 회귀 (Logistic Regression) 확률적 판별 모형 의사결정나무 (Decision Tree) 확률적 판별 모형 퍼셉트론 (Perceptron) 판별함수 모형 서포트 벡터 머신 (Support Vector Machine) 판별함수 모형 신경망 (Neural Network) 판별함수 모형 1. 확률적 모형1) 확률적 생성모형조건부확률 기반 생성모형의 장점 중 하나는 클래스가 3개 이상인 경우에도 바로 적용할 수 있다는 점이다. 생성모형은 더 구하기 쉬운 클래스별 특징 데이터의 확률분포 $ P(x | y = k) $, 즉 가능도를 먼저 추정한 다음 베이즈 정리를 사용하여 $ P(y = k | x) $ 를 계산한다. P(y = k | x) = \\dfrac{P(x | y = k)P(y = k)}{P(x)}또한 전체확률의 법칙을 이용하여 $ P(x) $ 를 구할 수 있다. 이 값을 알면 x라는 데이터만 입력되어도 그 데이터 자체가 정상적인 데이터인지 아닌지 판단할 수 있다. P(x) = \\sum^K_{k=1}P(x|y = k)P(y = k)확률적 생성모형의 예로 QDA와 Naive Bayesian model이 있다. QDA QDA(Quadratic Discriminant Analysis)에서는 다음과 같은 코드로 분류문제를 푼다. 1234from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysismodel = QuadraticDiscriminantAnalysis().fit(X, y)test_data = [[0.2, 0.2]]p = model.predict_proba(test_data) 나이브 베이지안 모형** TfidfVectorizer 전처리기는 텍스트 데이터를 BoW에 따라 실수 벡터로 변환한다. MultinomialNB 모형은 나이브 베이즈 방법으로 분류 문제를 예측한다. Pipeline을 사용하여 이 두 클래스 객체를 하나의 모형으로 합친다. 1234567891011from sklearn.datasets import fetch_20newsgroupsfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.pipeline import Pipelinenews = fetch_20newsgroups(subset=\"all\")model = Pipeline([('vect', TfidfVectorizer(stop_words=\"english\")),('nb', MultinomialNB()),])model.fit(news.data, news.target) ​ 20개의 클래스 중 3번 클래스에 가장 높은 조건부확률을 갖기 때문에 3번째 클래스에 해당한다고 판별한다.​​​​ #### 2) 확률적 판별 모형​​ 확률적 생성 모형과 달리 확률적 판별 모형에서는 조건부확률 $ p(y = 1 | x) $ 가 x값에 따라 0에서 1 사이에서 달라지는 값을 갖는 함수라고 가정하고, 그 함수를 직접 찾아낸다.​ ​ p(y = k | x) = f(x) ​​ 확률적 판별 모형에는 로지스틱 회귀모형과 의사결정나무가 있다.​​​​ - 로지스틱 회귀모형​​ 1234567​ from sklearn.datasets import make_classification​ from sklearn.linear_model import LogisticRegression​ ​ X0, y = make_classification(n_features=1, n_redundant=0,​ n_informative=1, n_clusters_per_class=1, random_state=4)​ model = LogisticRegression().fit(X0, y)​ ​​ ​​​​ ### 2. 판별함수 기반 모형​​ 판별함수 기반 모형은 클래스의 영역을 나누는 경계면 혹은 경계선 함수 $ f(x) $ 를 정의하고, 이 판별함수 값의 부호에 따라 클래스가 나뉘어진다.​ ​ \\text{판별 경계선} : f(x) = 0 ​ \\​ ​ \\text{클래스 1} : f(x) > 0 ​​ ​ \\text{클래스 0} : f(x) < 0 ​​ scikit-learn에서는 decision_function메서드를 통해 판별함수 값을 출력할 수 있다.​​ 판별함수기반 모형으로는 퍼셉트론과 커널 SVM이 있다.​​​​ - 퍼셉트론​​ 가장 단순한 판별함수 모형으로, 두 개의 클래스만 구분해낼 수 있으며 직선으로 구분되는 경계선만을 찾아낸다.​​ 123456789​ from sklearn.linear_model import Perceptron​ from sklearn.datasets import load_iris​ iris = load_iris()​ idx = np.in1d(iris.target, [0, 2])​ X = iris.data[idx, 0:2]​ y = iris.target[idx]​ ​ model = Perceptron(max_iter=100, eta0=0.1, random_state=1).fit(X, y)​ ​​ ​​ ​​ 만약 데이터가 3차원이라면 경계선이 아닌 경계면(boundary surface)를 갖게 된다. 경계면 혹은 경계선을 decision hyperplane 이라고 한다.​​ ​​​​ - 커널 SVM​​ 직선인 경계면밖에 구분하지 못하는 퍼셉트론과 달리 곡선인 경계면을 찾아낼수 있다.​​ 123456789101112​ from sklearn import svm​ ​ xx, yy = np.meshgrid(np.linspace(-3, 3, 500),​ np.linspace(-3, 3, 500))​ np.random.seed(0)​ X = np.random.randn(300, 2)​ Y = np.logical_xor(X[:, 0] &gt; 0, X[:, 1] &gt; 0)​ ​ model = svm.NuSVC().fit(X, Y)​ Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])​ Z = Z.reshape(xx.shape)​ ​​ ​​ ​","categories":[{"name":"Math","slug":"Math","permalink":"https://jyujin39.github.io/categories/Math/"},{"name":"Classification","slug":"Math/Classification","permalink":"https://jyujin39.github.io/categories/Math/Classification/"}],"tags":[{"name":"study","slug":"study","permalink":"https://jyujin39.github.io/tags/study/"}]},{"title":"multi-class classification","slug":"multi-class-classification","date":"2018-11-28T12:17:36.000Z","updated":"2018-11-29T11:42:00.091Z","comments":true,"path":"2018/11/28/multi-class-classification/","link":"","permalink":"https://jyujin39.github.io/2018/11/28/multi-class-classification/","excerpt":"","text":"다중 클래스 분류 이진(Binary Class) 분류 : 종속변수의 클래스가 2개인 분류 문제 다중 클래스(Multi-Class) 분류 : 종속변수의 클래스가 3 개 이상인 분류문제 OvO 혹은 OvR 방법을 통해 여러 개의 이진 클래스 분류문제로 변환해서 푼다 OvO (One-vs-One): K개의 타겟 클래스가 존재할 때, 그 중 2개씩 선택해 이진 클래스 분류 문제를 풀고, 그 결과로 가장 많은 판별값을 얻은 클래스를 선택하는 방법 풀어야 하는 이진 클래스분류 문제의 수 : $ _KC_2 $ 두 개의 클래스씩 비교했을 때 선택받은 횟수의 총합으로 비교하면 여러 클래스가 동점이 나오는 tie case가 발생할 수 있기 때문에 각 클래스가 얻은 조건부 확률값을 모두 더한 값으로 비교하면 그 문제가 해결된다. OneVsOneClassifier 클래스를 사용하면 이진 클래스용 모형을 OvO 방법으로 다중 클래스용 모형으로 변환한다. 12345from sklearn.multiclass import OneVsOneClassifierfrom sklearn.linear_model import LogisticRegressionmodel_ovo = OneVsOneClassifier(LogisticRegression()).fit(iris.data, iris.target) #2진 모형 인스턴스를 만들고 OvO로 wrapping하면 내부에서 세 번 경합 실시 각 클래스가 얻는 조건부 확률값을 합한 값을 decision_function으로 출력한다. 12345678ax1 = plt.subplot(211)pd.DataFrame(model_ovo.decision_function(iris.data)).plot(ax=ax1, legend=False)plt.title(\"판별 함수\")ax2 = plt.subplot(212)pd.DataFrame(model_ovo.predict(iris.data), columns=[\"prediction\"]).plot(marker='o', ls=\"\", ax=ax2)plt.title(\"클래스 판별\")plt.tight_layout()plt.show() ​ 0(파란색), 1(주황색), 2(초록색)의 총 3개 클래스로 데이터가 판별되었고, 총 세 개의 데이터가 잘못 예측되었음이 확인 가능하다. OvR (One-vs-the-Rest): 클래스 개수가 K개이면 풀어야할 이진분류 문제가 K의 제곱에 비례하여 많아지는 OvO와 달리, OvR은 K개의 문제를 풀면 되기 때문에 훨씬 빠르고 효율적이다. 클래스 A, B, C가 있을 때, A vs B,C 즉, A vs A$^C$ B vs A,C 즉, B vs B$^C$ C vs A,B 즉, C vs C$^C$ 이렇게 3 번 이진문제를 푸는데, OvR에서도 판별 결과의 수가 같은 동점 문제가 발생할 수가 있기 때문에 각 클래스가 얻은 조건부 확률값을 더하여 그 값이 +가 나오면 해당 클래스고 -가 나오면 해당 클래스가 아니라고 판단한다. 결과적으로는 +값이 나온 클래스들 중 가장 그 값이 큰 클래스를 정답으로 예측한다. OneVsRestClassifier 클래스를 사용하면 이진 클래스용 모형을 OvR 방법으로 다중 클래스용 모형으로 변환한다. 12345678910111213from sklearn.multiclass import OneVsRestClassifierfrom sklearn.linear_model import LogisticRegressionmodel_ovr = OneVsRestClassifier(LogisticRegression()).fit(iris.data, iris.target)ax1 = plt.subplot(211)pd.DataFrame(model_ovr.decision_function(iris.data)).plot(ax=ax1, legend=False)plt.title(\"판별 함수\")ax2 = plt.subplot(212)pd.DataFrame(model_ovr.predict(iris.data), columns=[\"prediction\"]).plot(marker='o', ls=\"\", ax=ax2)plt.title(\"클래스 판별\")plt.tight_layout()plt.show() ​ 그래프를 보면 클래스 판별 예측에 실패한 데이터의 수가 약 6개로 OvO보다 예측 성능이 조금 떨어지는 것을 볼 수 있다. 하지만 현실적으로 클래스가 많아지면 OvO는 아예 쓸 수가 없기 때문에 OvR을 쓰는 것이 일반적이다.","categories":[{"name":"Math","slug":"Math","permalink":"https://jyujin39.github.io/categories/Math/"},{"name":"Classification","slug":"Math/Classification","permalink":"https://jyujin39.github.io/categories/Math/Classification/"}],"tags":[{"name":"study","slug":"study","permalink":"https://jyujin39.github.io/tags/study/"}]}]}