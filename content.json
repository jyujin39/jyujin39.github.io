{"meta":{"title":"Data Science YJ","subtitle":"my daily study blog for Data Science","description":null,"author":"Yujin Jeon","url":"https://jyujin39.github.io"},"pages":[],"posts":[{"title":"QDA & LDA","slug":"QDA_LDA","date":"2018-12-05T15:00:00.000Z","updated":"2018-12-06T13:17:13.542Z","comments":true,"path":"2018/12/06/QDA_LDA/","link":"","permalink":"https://jyujin39.github.io/2018/12/06/QDA_LDA/","excerpt":"","text":"QDA와 LDA확률론적 생성모형에서는 베이즈 정리를 사용하여 조건부확률을 계산한다고 했다. P(y=k\\mid x) = \\dfrac{P(x\\mid y=k)P(y=k)}{P(x)}하나의 독립변수 x에 대해 y가 k일 경우의 조건부확률을 모두 구해서 그 중 가장 값이 큰 y로 추정하는데, 위 베이즈정리 공식에서 분모는 P(x)이므로 이 때 분모값은 고정이다. 따라서 확률의 크기만을 비교해도 되는 경우에는 현실적으로 분모를 따로 구하지 않고 분자만을 계산해 비교하여 클래스를 판별하기도 한다. P(y=k\\mid x) \\; \\; \\propto \\;\\; P(x\\mid y=k)P(y=k)여기서 사전확률, 즉 $P(y=k)$ 는 다음처럼 계산한다. P(y=k) \\approx \\dfrac{\\;\\;\\;y=k\\text{인 데이터의 수}\\;\\;\\;}{모든 데이터의 수}그리고 가능도 $P(x \\mid y=k)$ 는 다음과 같이 계산한다. $P(x \\mid y= k)$ 가 특정한 확률분포 모형을 따른다고 가정한다. k번째 클래스에 속하는 학습데이터 {$x_1, \\cdots , x_{N}$} 을 사용하여 이 모형의 모수 값을 구한다. 모수값을 알고 있으므로 $P(x \\mid y=k)$ 의 확률밀도함수를 구한 것이다. 즉, 새로운 독립변수가 어떤 x가 되더라도 가능도를 구할 수 있다. QDA베이즈 정리를 사용하여 조건부확률 $p(y=k\\mid x)$ 을 계산하는 확률적 생성모형 중에서, 독립변수 x가 다변수 가우시안 정규분포(Multivariable Gaussian Normal distribution)을 따른다고 가정하는 모형이 QDA(Quadratic Discriminant Analysis)이다. p(x\\mid y=k) = \\dfrac{1}{(2\\pi)^{D/2}|\\Sigma_k|^{1/2}}\\text{exp}\\left(-\\dfrac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k)\\right) $\\sum_k$ : k클래스에 해당하는 x들의 공분산행렬 $\\mu_k$ : 1클래스에 해당하는 x들의 평균 이 분포들을 알고 있으면 독립변수 x에 대한 y의 조건부확률 분포는 다음과 같이 베이즈 정리와 전체확률의 법칙으로 구할 수 있다. P(y=k\\mid x) = \\dfrac{p(x\\mid y=k)P(y=k)}{p(x)}=\\dfrac{p(x\\mid y=k)P(y=k)}{\\sum_l p(x\\mid y=l)P(y=l)}예를 들어 y가 1, 2, 3 세 개의 클래스를 가지고, 각 클래스에서의 x의 확률분포가 다음과 같은 기댓값 및 공분산행렬을 갖는다고 가정하자. \\mu_1 = \\begin{bmatrix} 0\\\\0 \\end{bmatrix},\\;\\; \\mu_2 = \\begin{bmatrix} 1\\\\1 \\end{bmatrix}, \\;\\;\\mu_3 = \\begin{bmatrix} -1\\\\1 \\end{bmatrix}\\\\ \\Sigma_1 = \\begin{bmatrix} 0.7 & 0\\\\0 & 0.7 \\end{bmatrix},\\;\\; \\Sigma_2 = \\begin{bmatrix} 0.8 & 0.2\\\\0.2 & 0.8 \\end{bmatrix}, \\;\\; \\Sigma_3 = \\begin{bmatrix} 0.8 & 0.2\\\\0.2 & 0.8 \\end{bmatrix}y의 사전확률은 다음과 같이 동일하다. P(Y = 1) = P(Y=2) = P(Y=3) = \\dfrac{1}{3}Scikit-Learn은 QDA 모형을 위한QuadraticDiscriminantAnalysis 클래스를 제공한다. 이 클래스로 위에서 가정한 바에 따라 모형을 만들어 보자. 1234567891011121314N = 100np.random.seed(0)X1 = sp.stats.multivariate_normal([0, 0], [[0.7, 0], [0, 0.7]]).rvs(100)X2 = sp.stats.multivariate_normal([1, 1], [[0.8, 0.2], [0.2, 0.8]]).rvs(100)X3 = sp.stats.multivariate_normal([-1, 1], [[0.8, 0.2], [0.2, 0.8]]).rvs(100)y1 = np.zeros(N)y2 = np.ones(N)y3 = 2*np.ones(N)X = np.vstack([X1, X2, X3])y = np.hstack([y1, y2, y3])from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysisqda = QuadraticDiscriminantAnalysis(store_covariance=True).fit(X, y)#store_covariance=True로 놓으면 공분산행렬을 제공한다 123456qda.means_ #각 클래스에서의 추정된 기댓값 벡터 #결과array([[-8.01254084e-04, 1.19457204e-01], # class 1일 때 [ 1.16303727e+00, 1.03930605e+00], # 2일 때 [-8.64060404e-01, 1.02295794e+00]])# 3일 때 1234567qda.covariance_ #공분산행렬#결과[array([[ 0.73846319, -0.01762041], [-0.01762041, 0.72961278]]), array([[0.66534246, 0.21132313], [0.21132313, 0.78806006]]), array([[0.9351386 , 0.22880955], [0.22880955, 0.79142383]])] 이 확률분포를 사용하여 분류를 한 결과는 다음과 같다. 1234567891011121314151617x1min, x1max = -5, 5x2min, x2max = -4, 5XX1, XX2 = np.meshgrid(np.arange(x1min, x1max, (x1max-x1min)/1000), np.arange(x2min, x2max, (x2max-x2min)/1000))YY = np.reshape(qda.predict(np.array([XX1.ravel(), XX2.ravel()]).T), XX1.shape)cmap = mpl.colors.ListedColormap(sns.color_palette([\"r\", \"g\", \"b\"]).as_hex())plt.contourf(XX1, XX2, YY, cmap=cmap, alpha=0.5)plt.scatter(X1[:, 0], X1[:, 1], alpha=0.8, s=50, marker=\"o\", color='r', label=\"클래스 1\")plt.scatter(X2[:, 0], X2[:, 1], alpha=0.8, s=50, marker=\"s\", color='g', label=\"클래스 2\")plt.scatter(X3[:, 0], X3[:, 1], alpha=0.8, s=50, marker=\"x\", color='b', label=\"클래스 3\")plt.xlim(x1min, x1max)plt.ylim(x2min, x2max)plt.xlabel(\"x1\")plt.ylabel(\"x2\")plt.title(\"QDA 분석 결과\")plt.legend()plt.show() 이 그래프는 predict 결과를 등고선으로 나타낸 것이다. 경계선이 그려진 영역에서 높이가 구분되고, 다른 곳은 높이가 일정한 평지이다. 각 영역에 다른 색깔로 misclassification된 데이터들이 많지만 그건 어쩔 수가 없다. QDA의 약점 모형을 만들어 예측하려면 독립변수들의 Covariance 행렬을 먼저 추정해내야 한다는 점이다. 데이터가 적을 때는 괜찮지만 현실에서 데이터가 수천 수만개가 되면, 공분산 행렬의 크기는 데이터수의 제곱만큼 커지게 된다. 데이터가 많아질수록 공분산행렬에는 노이즈가 많아지고, 추정도 부정확하게 된다. LDA(Linear discriminant analysis)LDA(Linear Discriminant Analysis)에서는 클래스별로 중심의 위치만 다를 뿐 데이터의 분포는 같다고 가정한다. 즉, 각 클래스 y에 대한 독립변수 x의 조건부 확률분포가 공통된 공분산 행렬을 갖는 다변수 가우시안 정규분포라고 가정한다. \\Sigma_k = \\Sigma \\;\\;\\;\\;\\text{for all} \\;\\;k이러한 가정은 현실과는 다를 수 있지만 데이터 자체에 노이즈가 섞이는 것을 막아주어, 분류가 qda보다 오히려 정확하게 이루어질 수 있다. 각 class 영역을 구분하는 경계가 곡선이었던 QDA와 달리 LDA에서는 직선이 된다. Scikit-Learn은 LDA 모형을 위한 LinearDiscriminantAnalysis 클래스를 제공한다. 아래 사용한 데이터는 위 QDA를 진행했던 것과 같은 데이터다. 12from sklearn.discriminant_analysis import LinearDiscriminantAnalysislda = LinearDiscriminatAnalysis(n_components=3, solver='svd', store_covariance=True).fit(X, y) LDA에서는 기댓값 벡터만 클래스에 따라 달라지고, 공분산행렬은 모든 클래스에 대해 하나로 동일하다. 12345lda.means_#결과array([[-8.01254084e-04, 1.19457204e-01], [ 1.16303727e+00, 1.03930605e+00], [-8.64060404e-01, 1.02295794e+00]]) 1234lda.covariance_#결과array([[0.7718516 , 0.13942905], [0.13942905, 0.7620019 ]]) LDA 모형에 따른 분류 결과는 다음과 같다. 1234567891011121314151617x1min, x1max = -5, 5x2min, x2max = -4, 5XX1, XX2 = np.meshgrid(np.arange(x1min, x1max, (x1max-x1min)/1000), np.arange(x2min, x2max, (x2max-x2min)/1000))YY = np.reshape(lda.predict(np.array([XX1.ravel(), XX2.ravel()]).T), XX1.shape)cmap = mpl.colors.ListedColormap(sns.color_palette([\"r\", \"g\", \"b\"]).as_hex())plt.contourf(XX1, XX2, YY, cmap=cmap, alpha=0.5)plt.scatter(X1[:, 0], X1[:, 1], alpha=0.8, s=50, marker=\"o\", color='r', label=\"클래스 1\")plt.scatter(X2[:, 0], X2[:, 1], alpha=0.8, s=50, marker=\"s\", color='g', label=\"클래스 2\")plt.scatter(X3[:, 0], X3[:, 1], alpha=0.8, s=50, marker=\"x\", color='b', label=\"클래스 3\")plt.xlim(x1min, x1max)plt.ylim(x2min, x2max)plt.xlabel(\"x1\")plt.ylabel(\"x2\")plt.legend()plt.title(\"LDA 분석 결과\")plt.show() LDA의 약점 문제는, LDA에서도 데이터가 너무 많아지면 공분산행렬이 너무 커진다는 점이다. 그래서 나온 가정이 공분산행렬의 대각성분만 구하고 나머지 비대각성분은 0이라고 하자는 나이브 가정이다. QDA를 간략화한 모형이 LDA이고, 그걸 더 간략화한 모형이 다음 설명할 나이브베이즈 모형이다. 참조:","categories":[{"name":"Math","slug":"Math","permalink":"https://jyujin39.github.io/categories/Math/"},{"name":"Classification","slug":"Math/Classification","permalink":"https://jyujin39.github.io/categories/Math/Classification/"}],"tags":[{"name":"study","slug":"study","permalink":"https://jyujin39.github.io/tags/study/"}]},{"title":"classification performance evaluation","slug":"classification_performance_evaluation","date":"2018-12-01T15:00:00.000Z","updated":"2018-12-04T06:49:37.092Z","comments":true,"path":"2018/12/02/classification_performance_evaluation/","link":"","permalink":"https://jyujin39.github.io/2018/12/02/classification_performance_evaluation/","excerpt":"","text":"분류 성능 평가분류 문제는 회귀분석과 달리 다양한 성능 평가기준이 필요하다. Scikit-Learn에서 제공하는 분류 성능평가 메서드들은 다음과 같다. sklearn.metrics 서브 패키지 confusion_matrix() classfication_report() accuracy_score(y_true, y_pred) precision_score(y_true, y_pred) recall_score(y_true, y_pred) fbeta_score(y_true, y_pred, beta) f1_score(y_true, y_pred) 분류 결과표 Confusion Matrix분류 결과표는 타겟의 실제 클래스와 모형이 예측한 클래스가 일치하는 개수를 표로 나타낸 것이다. 원래 클래스는 행으로, 예측한 클래스는 열로 나타낸다. 예측 클래스 0 예측 클래스 1 예측 클래스 2 원래 클래스 0 2 0 0 원래 클래스 1 0 0 1 원래 클래스 2 1 0 2 이를 코드로 구현하면 다음과 같다. 1234from sklearn.metrics import confusion_matrixy_true = [2, 0, 2, 2, 0, 1]y_pred = [0, 0, 2, 2, 0, 2]confusion_matrix(y_true, y_pred) 1234#분류결과표array([[2, 0, 0], [0, 0, 1], [1, 0, 2]]) 이진 분류 결과표 Binary Confusion Matrix클래스가 두 개인 경우에는 일반적으로 클래스 이름을 “Positive”와 “Negative”로 표시한다. positive : 특이한 케이스에 해당하는 경우 negative : 그렇지 않은 일반적인 경우 이진 분류 결과표는 다음과 같은 모양이다. Positive라고 예측 Negative라고 예측 실제 Positive True Positive False Negative 실제 Negative False Positive True Negative FDS (Fraud Detection System)잘못된 거래 및 사기 거래 등을 예측하는 시스템으로, 예측 결과가 실제와 일치하는지에 따라 다음과 같이 구분한다. TP(True Positive): 사기를 사기라고 정확하게 예측 TN(True Negative): 정상을 정상이라고 정확하게 예측 FP(False Positive): 정상을 사기라고 잘못 예측 FN(False Negative): 사기를 정상이라고 잘못 예측 FDS는 병의 진단에도 쓰일 수 있다. (positive: 병에 걸린 것, negative: 병에 걸리지 않은 것) 이진분류결과표에 따르면 분류모델의 성능평가척도가 4개로 추려지지만 그래도 여전히 많다. 그래서 그 4개의 수를 조합해 하나의 점수로 만든 것이 아래 설명할 평가점수다. 평가 점수FDS 의 결과로 나온 TP, TN, FP, FN 네 가지를 조합해 다음의 평가점수들을 계산한다. accuracy (정확도) precision (정밀도) recall (재현율) fallout (위양성율) F (beta) score 1) Accuracy​ : 전체 샘플 중 맞게 예측한 샘플 수의 비율 \\text{accuracy} = \\dfrac{TP + TN}{TP + TN + FP + FN}2) Precision​ : Positive 클래스라고 예측한 데이터 중 실제로 Positive에 해당하는 데이터의 비율 사기거래 추정에서 일단 positive라고 추정이 됐으면 그게 실제 사기문제일 수 있기 때문에 중요하게 여겨지고, 사기거래 특정 시스템이 실제로 잘 작동하고 있는지 파악할 수 있어야 되기 때문에 precision점수가 중요하게 된다. \\text{precision} = \\dfrac{TP}{TP + FP}3) Recall​ : 실제 Positive인 데이터 중 Positive라고 예측된 데이터의 비율 \\text{recall} = \\dfrac{TP}{TP + FN}4) Fall-Out​ : 실제 Negative인 데이터 중 Positive로 잘못 에측된 표본의 비율 \\text{fallout} = \\dfrac{FP}{FP + TN}5) F (beta) score​ :반비례관계에 있는 precision과 recall은 둘 다 중요한 점수이기 때문에 같이 봐야 하는데, 그 때 그 두 점수를 가중조화 평균낸 점수가 F-score다. 베타 값에 따라 달라진다. F_\\beta = \\dfrac{(1 + \\beta^2) \\, ({\\text{precision} \\times \\text{recall}})}{({\\beta^2 \\, \\text{precision} + \\text{recall}})} F1 score (beta = 1) F_1 = \\dfrac{2\\cdot\\text{precision}\\cdot\\text{recall}}{\\text{precision} + \\text{recall}} Scikit-Learn의 metrics 패키지에서는 정밀도, 재현율, F1-score를 구하는 classification_report 명령을 제공한다. 이 명령은 각각의 클래스를 positive로 보았을 때의 precision, recall, F1-score를 구하고 그 평균값으로 전체 모형의 성능을 평가한다. 123456from sklearn.metrics import classification_reporty_true = [0, 0, 0, 1, 1, 0, 0]y_pred = [0, 0, 0, 0, 1, 1, 1]print(classification_report(y_true, y_pred, target_names=[&apos;class 0&apos;, &apos;class 1&apos;])) 1234567#결과 precision recall f1-score support class 0 0.75 0.60 0.67 5 class 1 0.33 0.50 0.40 2avg / total 0.63 0.57 0.59 7 위의 평가 점수들은 서로 밀접한 관계를 맺고 있다. 재현율(recall)과 위양성률(fall-out)은 양의 상관 관계가 있다. 정밀도(precision)와 재현율(recall)은 대략적으로 음의 상관 관계가 있다. Precision vs. Recall분류모델 예측결과 precision과 recall이 모두 높으면 좋지만, 사실 두 점수는 반비례하는 경향이 있다. precision은, 예를들어 의사가 진단을 하는 경우 의사의 권위 및 능력과 직결되는 점수이다. precision이 낮으면 신뢰도가 떨어지기때문에 의사는 precision을 높이기 위해 판별함수 f(x)의 기준점을 0보다 높게 설정하게 된다. f(x)가 10 이상인 경우에만 positive라고 진단해버림으로써 0이상 10 미만일 때 오진이었던 경우를 배제해버리는 것이다. 그러면 precision점수가 높아진다. 이와 반대로 recall을 높이려면 존재하는 모든 positive를 잡아내야만 하는 것이 목표가 된다. 그러기 위해서는 반대로 f(x)의 임계점을 0보다 낮게 만든다. 일단 negative인 것들도 막 잡아내고 보면 positive가 다 잡히게 되어있기 때문이다. 따라서 precision과 recall을 동시에 높이기는 쉽지 않다. ROC 커브ROC(Receiver Operator Characteristic) 커브란, fall-out과 recall 값이 판별함수 기준값의 변화에 따라 어떻게 달라지는지를 시각화한 것이다. 아래 표는, 16개의 데이터에 대해 판별함수 기준값을 0으로 설정하고 이진분류를 진행한 결과이다. ​ 표를 보면 6번, 7번, 10번 데이터의 예측에 실패했음을 알 수 있다. 이 때 만약 판별함수 기준값을 6번데이터의 판별함수값인 0.244729보다 높이게 되면 6번데이터는 0클래스로 예측되게 되므로 정확한 예측이 된다. 이런 식으로 기준값을 높이거나 낮춰가면서 recall과 fall-out 점수를 확인하는 과정을 자동으로 진행하고 시각화해주는 것이 scikit-learn의 roc_curve 명령이다. ​ 아래 그래프는 위 16개의 데이터에 대한 ROC 곡선이다. 데이터가 더 많으면 더 곡선에 가까운 형태의 그래프가 그려진다. AUC분류모델은 Fall-Out 점수가 낮으면서 Recall이 높으면 좋기 때문에 ROC커브 그래프에서는 좌측 상단의 점이 높게 그려질수록 좋은 모델에 해당한다. 따라서 곡선 아래 면적이 클수록 좋은데, 그 면적을 측정한 것이 AUC(Area Under the Curve)이다. AUC가 1에 가까울수록 좋은 모델이라고 볼 수 있다. 12from sklearn.metrics import aucauc(fpr, tpr) 12#AUC 결과값0.9112016520622872","categories":[{"name":"Math","slug":"Math","permalink":"https://jyujin39.github.io/categories/Math/"},{"name":"Classification","slug":"Math/Classification","permalink":"https://jyujin39.github.io/categories/Math/Classification/"}],"tags":[{"name":"study","slug":"study","permalink":"https://jyujin39.github.io/tags/study/"}]},{"title":"multi-class classification","slug":"multi-class-classification","date":"2018-11-29T11:37:09.440Z","updated":"2018-11-29T11:57:04.846Z","comments":true,"path":"2018/11/29/multi-class-classification/","link":"","permalink":"https://jyujin39.github.io/2018/11/29/multi-class-classification/","excerpt":"","text":"다중 클래스 분류 이진(Binary Class) 분류 : 종속변수의 클래스가 2개인 분류 문제 다중 클래스(Multi-Class) 분류 : 종속변수의 클래스가 3 개 이상인 분류문제 OvO 혹은 OvR 방법을 통해 여러 개의 이진 클래스 분류문제로 변환해서 푼다 OvO (One-vs-One): K개의 타겟 클래스가 존재할 때, 그 중 2개씩 선택해 이진 클래스 분류 문제를 풀고, 그 결과로 가장 많은 판별값을 얻은 클래스를 선택하는 방법 풀어야 하는 이진 클래스분류 문제의 수 : $ _KC_2 $ 두 개의 클래스씩 비교했을 때 선택받은 횟수의 총합으로 비교하면 여러 클래스가 동점이 나오는 tie case가 발생할 수 있기 때문에 각 클래스가 얻은 조건부 확률값을 모두 더한 값으로 비교하면 그 문제가 해결된다. OneVsOneClassifier 클래스를 사용하면 이진 클래스용 모형을 OvO 방법으로 다중 클래스용 모형으로 변환한다. 12345from sklearn.multiclass import OneVsOneClassifierfrom sklearn.linear_model import LogisticRegressionmodel_ovo = OneVsOneClassifier(LogisticRegression()).fit(iris.data, iris.target) #2진 모형 인스턴스를 만들고 OvO로 wrapping하면 내부에서 세 번 경합 실시 각 클래스가 얻는 조건부 확률값을 합한 값을 decision_function으로 출력한다. 12345678ax1 = plt.subplot(211)pd.DataFrame(model_ovo.decision_function(iris.data)).plot(ax=ax1, legend=False)plt.title(\"판별 함수\")ax2 = plt.subplot(212)pd.DataFrame(model_ovo.predict(iris.data), columns=[\"prediction\"]).plot(marker='o', ls=\"\", ax=ax2)plt.title(\"클래스 판별\")plt.tight_layout()plt.show() ​ 0(파란색), 1(주황색), 2(초록색)의 총 3개 클래스로 데이터가 판별되었고, 총 세 개의 데이터가 잘못 예측되었음이 확인 가능하다. OvR (One-vs-the-Rest): 클래스 개수가 K개이면 풀어야할 이진분류 문제가 K의 제곱에 비례하여 많아지는 OvO와 달리, OvR은 K개의 문제를 풀면 되기 때문에 훨씬 빠르고 효율적이다. 클래스 A, B, C가 있을 때, A vs B,C 즉, A vs A$^C$ B vs A,C 즉, B vs B$^C$ C vs A,B 즉, C vs C$^C$ 이렇게 3 번 이진문제를 푸는데, OvR에서도 판별 결과의 수가 같은 동점 문제가 발생할 수가 있기 때문에 각 클래스가 얻은 조건부 확률값을 더하여 그 값이 +가 나오면 해당 클래스고 -가 나오면 해당 클래스가 아니라고 판단한다. 결과적으로는 +값이 나온 클래스들 중 가장 그 값이 큰 클래스를 정답으로 예측한다. OneVsRestClassifier 클래스를 사용하면 이진 클래스용 모형을 OvR 방법으로 다중 클래스용 모형으로 변환한다. 12345678910111213from sklearn.multiclass import OneVsRestClassifierfrom sklearn.linear_model import LogisticRegressionmodel_ovr = OneVsRestClassifier(LogisticRegression()).fit(iris.data, iris.target)ax1 = plt.subplot(211)pd.DataFrame(model_ovr.decision_function(iris.data)).plot(ax=ax1, legend=False)plt.title(\"판별 함수\")ax2 = plt.subplot(212)pd.DataFrame(model_ovr.predict(iris.data), columns=[\"prediction\"]).plot(marker='o', ls=\"\", ax=ax2)plt.title(\"클래스 판별\")plt.tight_layout()plt.show() ​ 그래프를 보면 클래스 판별 예측에 실패한 데이터의 수가 약 6개로 OvO보다 예측 성능이 조금 떨어지는 것을 볼 수 있다. 하지만 현실적으로 클래스가 많아지면 OvO는 아예 쓸 수가 없기 때문에 OvR을 쓰는 것이 일반적이다.","categories":[{"name":"Math","slug":"Math","permalink":"https://jyujin39.github.io/categories/Math/"},{"name":"Classification","slug":"Math/Classification","permalink":"https://jyujin39.github.io/categories/Math/Classification/"}],"tags":[{"name":"study","slug":"study","permalink":"https://jyujin39.github.io/tags/study/"}]},{"title":"classification models","slug":"classification-models","date":"2018-11-27T15:00:00.000Z","updated":"2018-12-03T04:04:47.190Z","comments":true,"path":"2018/11/28/classification-models/","link":"","permalink":"https://jyujin39.github.io/2018/11/28/classification-models/","excerpt":"","text":"분류모형분류(classification)는 독립 변수 값이 주어졌을 때 그 독립 변수 값과 가장 연관성이 큰 종속변수 카테고리(클래스)를 계산하는 문제이다. 분류 모형의 종류 판별함수(discriminant function) 모형 : 주어진 데이터를 서로 다른 영역으로 나누는 경계면을 찾는다. 확률적 모형 확률적 판별(discriminative) 모형 : 주어진 데이터가 특정 카테고리일 조건부확률을 직접 계산한다. 확률적 생성(generative) 모형 : 주어진 데이터가 특정 카테고리일 조건부확률을 베이즈정리를 통해 계산한다. 모형 방법론 Linear/Quadratic Discriminant Analysis 확률적 생성 모형 나이브 베이지안 (Naive Bayes) 확률적 생성 모형 로지스틱 회귀 (Logistic Regression) 확률적 판별 모형 의사결정나무 (Decision Tree) 확률적 판별 모형 퍼셉트론 (Perceptron) 판별함수 모형 서포트 벡터 머신 (Support Vector Machine) 판별함수 모형 신경망 (Neural Network) 판별함수 모형 1. 확률적 모형1) 확률적 생성모형조건부확률 기반 생성모형의 장점 중 하나는 클래스가 3개 이상인 경우에도 바로 적용할 수 있다는 점이다. 생성모형은 더 구하기 쉬운 클래스별 특징 데이터의 확률분포 $ P(x | y = k) $, 즉 가능도를 먼저 추정한 다음 베이즈 정리를 사용하여 $ P(y = k | x) $ 를 계산한다. P(y = k | x) = \\dfrac{P(x | y = k)P(y = k)}{P(x)}또한 전체확률의 법칙을 이용하여 $ P(x) $ 를 구할 수 있다. 이 값을 알면 x라는 데이터만 입력되어도 그 데이터 자체가 정상적인 데이터인지 아닌지 판단할 수 있다. P(x) = \\sum^K_{k=1}P(x|y = k)P(y = k)확률적 생성모형의 예로 QDA와 Naive Bayesian model이 있다. QDA QDA(Quadratic Discriminant Analysis)에서는 다음과 같은 코드로 분류문제를 푼다. 1234from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysismodel = QuadraticDiscriminantAnalysis().fit(X, y)test_data = [[0.2, 0.2]]p = model.predict_proba(test_data) 나이브 베이지안 모형** TfidfVectorizer 전처리기는 텍스트 데이터를 BoW에 따라 실수 벡터로 변환한다. MultinomialNB 모형은 나이브 베이즈 방법으로 분류 문제를 예측한다. Pipeline을 사용하여 이 두 클래스 객체를 하나의 모형으로 합친다. 1234567891011from sklearn.datasets import fetch_20newsgroupsfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.pipeline import Pipelinenews = fetch_20newsgroups(subset=\"all\")model = Pipeline([ ('vect', TfidfVectorizer(stop_words=\"english\")), ('nb', MultinomialNB()),])model.fit(news.data, news.target) ​ 20개의 클래스 중 3번 클래스에 가장 높은 조건부확률을 갖기 때문에 3번째 클래스에 해당한다고 판별한다. 2) 확률적 판별 모형확률적 생성 모형과 달리 확률적 판별 모형에서는 조건부확률 $ p(y = 1 | x) $ 가 x값에 따라 0에서 1 사이에서 달라지는 값을 갖는 함수라고 가정하고, 그 함수를 직접 찾아낸다. p(y = k | x) = f(x)​ 확률적 판별 모형에는 로지스틱 회귀모형과 의사결정나무가 있다.​​​ - **로지스틱** **회귀모형** 123456from sklearn.datasets import make_classificationfrom sklearn.linear_model import LogisticRegressionX0, y = make_classification(n_features=1, n_redundant=0,n_informative=1, n_clusters_per_class=1, random_state=4)model = LogisticRegression().fit(X0, y) ​ ​​​ ### 2. 판별함수 기반 모형 ​ 판별함수 기반 모형은 클래스의 영역을 나누는 경계면 혹은 경계선 함수 $ f(x) $ 를 정의하고, 이 판별함수 값의 부호에 따라 클래스가 나뉘어진다.​ $$$$ ​ \\text{판별 경계선} : f(x) = 0 ​ \\​ $$$$$$ ​ \\text{클래스 1} : f(x) > 0 ​ $$$$$$ ​ $$$$ ​ \\text{클래스 0} : f(x) < 0 ​​ scikit-learn에서는 decision_function메서드를 통해 판별함수 값을 출력할 수 있다.​​ 판별함수기반 모형으로는 퍼셉트론과 커널 SVM이 있다.​​​​ - 퍼셉트론​​ 가장 단순한 판별함수 모형으로, 두 개의 클래스만 구분해낼 수 있으며 직선으로 구분되는 경계선만을 찾아낸다.​​ 123456789​ from sklearn.linear_model import Perceptron​ from sklearn.datasets import load_iris​ iris = load_iris()​ idx = np.in1d(iris.target, [0, 2])​ X = iris.data[idx, 0:2]​ y = iris.target[idx]​ ​ model = Perceptron(max_iter=100, eta0=0.1, random_state=1).fit(X, y)​ ​​ ​​ ​​ 만약 데이터가 3차원이라면 경계선이 아닌 경계면(boundary surface)를 갖게 된다. 경계면 혹은 경계선을 decision hyperplane 이라고 한다.​​ ​​​​ - 커널 SVM​​ 직선인 경계면밖에 구분하지 못하는 퍼셉트론과 달리 곡선인 경계면을 찾아낼수 있다.​​ 123456789101112​ from sklearn import svm​ ​ xx, yy = np.meshgrid(np.linspace(-3, 3, 500),​ np.linspace(-3, 3, 500))​ np.random.seed(0)​ X = np.random.randn(300, 2)​ Y = np.logical_xor(X[:, 0] &gt; 0, X[:, 1] &gt; 0)​ ​ model = svm.NuSVC().fit(X, Y)​ Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])​ Z = Z.reshape(xx.shape)​ ​​ ​​ ​","categories":[{"name":"Math","slug":"Math","permalink":"https://jyujin39.github.io/categories/Math/"},{"name":"Classification","slug":"Math/Classification","permalink":"https://jyujin39.github.io/categories/Math/Classification/"}],"tags":[{"name":"study","slug":"study","permalink":"https://jyujin39.github.io/tags/study/"}]}]}